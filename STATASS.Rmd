---
title: "STAT5003Assignment1"
author: "Zezheng Zhang"
date: "26/04/2019"
output: html_document
---

---
title: "STAT5003 Assignment1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Initialize
```{r}
library(ggplot2)
theme_update(plot.title = element_text(hjust = 0.5))

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Exploratory data analysis
### Clean Data
#### DonorInfo
1. Load DonorInfo and drop columns

```{r}
DonorInfo <- read.csv("DonorInformation.csv", header = TRUE, sep = ",", encoding = "UTF-8")
DonorInfo.clean <- DonorInfo[c('donor_id', 'age', 'sex', 'apo_e4_allele', 'education_years', 'age_at_first_tbi', 'num_tbi_w_loc', 'act_demented')]

dim(DonorInfo.clean)
summary(DonorInfo.clean)

ggplot()+geom_histogram(data=data.frame(x=DonorInfo$age_at_first_tbi), bins = 20, aes(x)) + labs(title="Histogram of Age at first TBI", x ="Age at first TBI", y = "Population")
```

2. Deal with N/A and categorical features
age, sex, apo_e4_allele, education_years, age_at_first_tbi, longest_loc_duration, num_tbi_w_loc

Categorical features:
Age: Linear from ages 72-89; binned at 90-94, 95-99, and 100+.
Sex: Gender
apo_e4_allele: Yes - at least one ApoE4 allele; No - no ApoE4 allele present
longest_loc_duration: Ordinal score of length of loss of consciousness: 0 (none or unknown), 1 (a few sec or <), 2 (min or <), 3 (1-2 min), 4 (3-5 min), 5 (6-9 min), 6 (10 min-1 hr), and 7 (> 1 hr).

Numerical features:
education_years: Number of years of education completed
age_at_first_tbi: Age at which first TBI with loss of consciousness was reported.
num_tbi_w_loc: Number of recorded TBI, ranging from 0 (control) to 3.

```{r}
#clean age
summary(DonorInfo$age)
age <- as.character(DonorInfo$age) 
age[DonorInfo$age=='90-94'] <- 92
age[DonorInfo$age=='95-99'] <- 97
age[DonorInfo$age=='100+'] <- 100
age <- as.numeric(age)

ggplot()+geom_histogram(data=data.frame(x=age), bins = 20, aes(x)) + labs(title="Histogram of age", x ="Age", y = "Population")
summary(age)
DonorInfo.clean$age <- age

#clean apo_e4_allele
summary(DonorInfo$apo_e4_allele)
apo_e4_allele <- as.numeric(DonorInfo$apo_e4_allele)-1
#0->N 1->N/A 2->Y
#all N/A to N
apo_e4_allele[apo_e4_allele==1] <- 0
apo_e4_allele[apo_e4_allele==2] <- 1
DonorInfo.clean$apo_e4_allele <- apo_e4_allele

DonorInfo.clean$sex <- as.numeric(DonorInfo$sex)-1
DonorInfo.clean$act_demented <- as.numeric(DonorInfo$act_demented)-1

```

3. Check cleaned data

```{r}
summary(DonorInfo.clean)
```

# create 10 fold cross validation based on patients
```{r}
library(caret)
fold <- createFolds(DonorInfo.clean$act_demented, k=10)
cv = rep(0,length(DonorInfo.clean$act_demented))
for (i in 1:10) {
  cv[fold[[i]]] = i
}
DonorInfo.clean['cv'] = cv
table(DonorInfo.clean$cv)
```


#### ProteinInfo
1. Load ProteinInfo and remove redundant columns

```{r}
ProteinInfo.raw <- read.csv("ProteinAndPathologyQuantifications.csv", header = TRUE, sep = ",", encoding = "UTF-8")
head(ProteinInfo.raw)
ProteinInfo <- ProteinInfo.raw[,c(-2,-4)]
head(ProteinInfo)
```

```{r}
colSums(is.na(ProteinInfo))

p1 <-ggplot()+geom_density(data=data.frame(x=ProteinInfo$isoprostane_pg_per_mg), na.rm = T, aes(x)) + labs(title="Density of isoprostane_pg_per_mg", x ="isoprostane_pg_per_mg", y = "Density")
p2 <-ggplot()+geom_density(data=data.frame(x=ProteinInfo$ihc_tau2_ffpe), na.rm = T, aes(x)) + labs(title="Density of ihc_tau2_ffpe", x ="ihc_tau2_ffpe", y = "Density")
p3 <-ggplot()+geom_density(data=data.frame(x=ProteinInfo$ihc_at8_ffpe), na.rm = T, aes(x)) + labs(title="Density of ihc_at8_ffpe", x ="ihc_at8_ffpe", y = "Density")

multiplot(p1, p2, p3, cols=1)

#replace na with column mean
for (i in seq(ncol(ProteinInfo))) {
  ind <- is.na(ProteinInfo[,i])
  ProteinInfo[ind, i] <- median(ProteinInfo[,i], na.rm = T)
}

#remove the column with 239 N/A
ProteinInfo1 <- ProteinInfo[,-which(colnames(ProteinInfo)=="isoprostane_pg_per_mg")]

colSums(is.na(ProteinInfo1))
```

#merge ProteinInfo with DonorInfo

```{r}
data1 <- merge(x = DonorInfo.clean, y = ProteinInfo1, by = "donor_id", all = TRUE)
head(data1)
```

#### FPKM with column sample information
1. read csv

```{r}
colsamp <- read.csv("columns-samples.csv", header = TRUE, sep = ",", encoding = "UTF-8")
dim(colsamp)
head(colsamp)
fpkm <- read.csv("fpkm_table_unnormalized.csv", header = TRUE, sep = ",", encoding = "UTF-8")
dim(fpkm)
fpkm[1:5,1:10]

#summary(colsamp[colsamp$structure_name=='temporal neocortex',]$structure_color)
```

2. remove redundant ids in colsamp

```{r}
colsamp <- colsamp[,c(1,2,8)]
head(colsamp)
```

3. merge colsamp with data1

```{r}
data2 <- merge(x = colsamp, y = data1, by = c("donor_id", "structure_id"))
dim(data2)
head(data2)
colnames(data2)

# reorder the dataframe based on rnaseq profile id
data2.new <- data2[order(data2[,'rnaseq_profile_id']),]
head(data2.new)

```

4. transpose fpkm

```{r}
fpkm1 <- as.data.frame(t(fpkm))[-1,]
colnames(fpkm1)<-paste("X", fpkm[,1], sep = "")
fpkm1[1:5, 1:10]
```

5. combine fpkm1 with data2.new to produce data.final

```{r}
data3 <- cbind(data2.new, fpkm1)
rownames(data3) <- rownames(fpkm1)

#remove ids
data.final <- data3[,-(1:3)]

data.final[1:5, 1:10]

```

```{r}
#plot(density(rowSums(fpkm1==0)))
#plot(density(colSums(fpkm1==0)))
#plot((rowSums(fpkm1==0)))
#plot(sort(colSums(fpkm1==0)))
#((rowSums(fpkm1==0)))
#((colSums(fpkm1==0)))
```

```{r}
# save cv info
cv.377 <- data.filter1['cv']

# save y values
y <- as.matrix(data.filter1['act_demented'])
```

## feature filtering

1. remove features with more than 250 0s. 
```{r}
# Check if there is any 0 columns
ind.zero <- which(colSums(dataF==0)>250)
data.filter1 <- data.final[, -ind.zero]
sum(colSums(dataFF)==0)
dim(data.filter1)
```

2. remove features with less than 0.001 correaltion with y 
```{r}
# correlation with y
feature.cor <- cor(data.filter1, y)
lowcorind <- which(abs(feature.cor)<0.01)
data.filter2 <- data.filter1[,-lowcorind]

# dimension of the filtered data
dim(data.filter2)

# sample of filtered feature names
colnames(data.filter1)[lowcorind[1:20]]
```

3. remove features with t test
```{r}
# perform a t-test
y.ind = which(names(data.filter2)=='act_demented')

data.byClass <- split(data.filter2[, -y.ind], data.filter2$act_demented)

feature.pvalues <- c()
for(i in 1:(ncol(data.filter2)-1)) {
  feature.pvalues <- c(feature.pvalues, t.test(data.byClass[[1]][,i], data.byClass[[2]][,i])$p.value)
}

names(feature.pvalues) <- colnames(data.filter2[, -y.ind])

ttest.filter.ind <- which(feature.pvalues>0.05)

data.filter3 <- data.filter2[, -y.ind][, -ttest.filter.ind]
dim(data.filter3)
```

```{r}
which(colnames(data.filter3)=="act_demented")
which(colnames(data.filter3)=="cv")
```

```{r}
data.numeric <- data.filter3
# library(Rtsne)
# data.Rtsne <- Rtsne(data.numeric)
# data.Rtsne.df <- data.frame(dim1 = data.Rtsne$Y[,1], dim2 = data.Rtsne$Y[,2], labels = as.factor(y))
# p1 <- ggplot(data.Rtsne.df, aes(dim1, dim2, col = labels)) + geom_point() + ggtitle("Perplexity = 30")
# p1
```

```{r warning = FALSE}
data.pca.scale <- prcomp(data.numeric, scale = T)
# biplot(data.pca.scale, cex = 0.5)
```

```{r}
pca.df <- as.data.frame(data.pca.scale$x)
pca.df['label']=as.factor(y)
ggplot(pca.df, aes(PC1, PC2, colour = label)) + geom_point() 
```


```{r}
library(ggplot2)
tot.var <- sum(data.pca.scale$sdev^2)
var.explained <- data.frame(pc = seq(1:length(data.pca.scale$sdev)), var.explained  = data.pca.scale$sdev^2/tot.var ) 
ggplot(var.explained, aes(pc, var.explained)) + geom_point()+ ggtitle("variance explained vs pc")
ggplot(var.explained, aes(pc, cumsum(var.explained))) + geom_point()+ ggtitle("cumsum var explained vs pc")
```

```{r}
acc <- function(y, y.pred){
  return(sum(y==y.pred)/length(y))
}
```


# logistic regression on PCA
```{r warning=FALSE}
library(e1071)
library(randomForest)

lr.acc.train.pc <- c()
lr.acc.val.pc <- c()

svm.acc.train.pc <- c()
svm.acc.val.pc <- c()

rf.acc.train.pc <- c()
rf.acc.val.pc <- c()

pc.no <- seq(10,370,60)

for (pc in pc.no) {
  lr.acc.train <- c()
  lr.acc.val <- c()
  svm.acc.train <- c()
  svm.acc.val <- c()
  rf.acc.train <- c()
  rf.acc.val <- c()
  
  for(i in 1:length(fold)){
    train.df = pca.df[!cv.377==i, c(seq(1,pc),378)]
    val.df = pca.df[cv.377==i, c(seq(1,pc),378)]
    y.ind = which(names(train.df)=='label')
    
    logistic.model <- glm(label ~ .  , data = train.df, family = binomial(link = 'logit'))
    lr.pred.train <- ifelse(predict(logistic.model, train.df) > 0.5, 1, 0)
    lr.pred.val <- ifelse(predict(logistic.model, val.df) > 0.5, 1, 0)
    lr.acc.train <- c(lr.acc.train, acc(lr.pred.train, train.df[,y.ind]))
    lr.acc.val <- c(lr.acc.val, acc(lr.pred.val, val.df[,y.ind]))
    
    svm.model <- svm(x=train.df[,-y.ind], y=train.df[,y.ind],  kernel="linear", type="C-classification", cost = 1)
    svm.pred.train <- predict(svm.model, train.df[,-y.ind])
    svm.pred.val <- predict(svm.model, val.df[,-y.ind])
    svm.acc.train <- c(svm.acc.train, acc(svm.pred.train, train.df[,y.ind]))
    svm.acc.val <- c(svm.acc.val, acc(svm.pred.val, val.df[,y.ind]))
    
    rf.model <- randomForest(label ~ ., data=train.df, importance=TRUE)
    rf.pred.train <- predict(rf.model, train.df)
    rf.pred.val <- predict(rf.model, val.df)
    rf.acc.train <- c(rf.acc.train, acc(rf.pred.train, train.df[,y.ind]))
    rf.acc.val <- c(rf.acc.val, acc(rf.pred.val, val.df[,y.ind]))
    
  }
  lr.acc.train.pc <- c(lr.acc.train.pc, mean(lr.acc.train))
  lr.acc.val.pc <- c(lr.acc.val.pc, mean(lr.acc.val))
  svm.acc.train.pc <- c(svm.acc.train.pc, mean(svm.acc.train))
  svm.acc.val.pc <- c(svm.acc.val.pc, mean(svm.acc.val))
  rf.acc.train.pc <- c(rf.acc.train.pc, mean(rf.acc.train))
  rf.acc.val.pc <- c(rf.acc.val.pc, mean(rf.acc.val))
}

```

```{r}
plot(pc.no, lr.acc.val.pc)
pc.no[which.max(lr.acc.val.pc)]
max(lr.acc.val.pc)

plot(pc.no, svm.acc.val.pc)
pc.no[which.max(svm.acc.val.pc)]
max(svm.acc.val.pc)

plot(pc.no, rf.acc.val.pc)
pc.no[which.max(rf.acc.val.pc)]
max(rf.acc.val.pc)
```

# forward wrapper approach to select the best combination of PCs
```{r}
selectFeature <- function(df, features, mode) {
  ## identify a feature to be selected
  current.best.accuracy <- -Inf
  selected.i <- NULL
  for(i in 1:(ncol(df)-1)) {
    current.f <- colnames(df)[i]
    if(!current.f %in% features) {
      test.acc = rep(0,10)
      for(cv in 1:length(fold)){
        train = df[!cv.377==cv, c(features, current.f)]
        test = df[cv.377==cv, c(features, current.f)]
        
        if(mode == "svm"){
          y.ind = which(colnames(train)=='label')
          svm.model <- svm(x=train[,-y.ind], y=train[,y.ind],  kernel="linear", type="C-classification", cost = 0.1)
          model <- predict(svm.model, test[,-y.ind])
        }
        else if(mode == "lr"){
          logistic.model <- glm(label ~ .  , data = train, family = binomial(link = 'logit'))
          model <- ifelse(predict(logistic.model, test) > 0.5, 1, 0)
        }
        
        test.acc[cv] <- acc(test$label, model)
      }
      if(mean(test.acc) > current.best.accuracy) {
        current.best.accuracy <- mean(test.acc)
        selected.i <- colnames(df)[i]
      }
    }
  }
  return(list(selected.i, current.best.accuracy))
}
```



## Forward stepwise + PCA + Logistic
```{r warning=FALSE}
features.pca.lr <- c('PC1', 'label')

lr.forward.acc <- c()

# select the 2 to 10 best features using knn as a wrapper classifier
for (j in 2:50) {
  ret <- selectFeature(pca.df, features.pca.lr, "lr")
  selected.i <- ret[[1]]
  current.best.acc <- ret[[2]]
  cat(j, selected.i, ': ', current.best.acc, '\n')

  # add the best feature from current run
  features.pca.lr <- c(features.pca.lr, selected.i)
  
  lr.forward.acc <- c(lr.forward.acc, current.best.acc)
}

```

# PCA + SVM + Forward stepwise
```{r}
features.pca.svm <- c('PC1', 'label')

svm.forward.acc <- c()

# select the 2 to 10 best features using knn as a wrapper classifier
for (j in 2:30) {
  ret <- selectFeature(pca.df, features.pca.svm, "svm")
  selected.i <- ret[[1]]
  current.best.acc <- ret[[2]]
  cat(j, selected.i, ': ', current.best.acc, '\n')

  # add the best feature from current run
  features.pca.svm <- c(features.pca.svm, selected.i)
  
  svm.forward.acc <- c(svm.forward.acc, current.best.acc)
}

plot(seq(2:30), svm.forward.acc)
```


# t-test + Logistic + Forward Stepwise
```{r}
features.ttest.lr = c('label', names(data.ttest)[1])
lr.forward.acc.ttest <- c()

for (j in 2:30) {
  ret <- selectFeature(data.ttest[,c(seq(1,400),16836)], features.ttest.lr, "lr")
  selected.i <- ret[[1]]
  current.best.acc <- ret[[2]]
  cat(j, selected.i, ': ', current.best.acc, '\n')

  # add the best feature from current run
  features.ttest.lr <- c(features.ttest.lr, selected.i)
  
  lr.forward.acc.ttest <- c(lr.forward.acc.ttest, current.best.acc)
}

plot(seq(2,30), lr.forward.acc.ttest)
```

# t-test + SVM + Forward stepwise
```{r}
features.ttest.svm = c('label', names(data.ttest)[1])

svm.forward.acc <- c()

# select the 2 to 10 best features using knn as a wrapper classifier
for (j in 2:50) {
  ret <- selectFeature(data.ttest[,c(seq(1,400),16836)], features.ttest.svm, "svm")
  selected.i <- ret[[1]]
  current.best.acc <- ret[[2]]
  cat(j, selected.i, ': ', current.best.acc, '\n')

  # add the best feature from current run
  features.ttest.svm <- c(features.ttest.svm, selected.i)
  
  svm.forward.acc <- c(svm.forward.acc, current.best.acc)
}

plot(seq(2:50), svm.forward.acc)
```







# Rank t-test and order features for selection
```{r}
data.byClass <- split(data.numeric, y)

feature.pvalues <- c()
for(i in 1:(ncol(data.numeric))) {
  feature.pvalues <- c(feature.pvalues, t.test(data.byClass[[1]][,i], data.byClass[[2]][,i])$p.value)
}

names(feature.pvalues) <- colnames(data.numeric)

# no small value of t-stats
ttest.filter.ind <- which(feature.pvalues>0.05)

ttest.feature.ind.ordered = order(feature.pvalues)

data.ttest <- data.numeric[,ttest.feature.ind.ordered]
data.ttest['label'] = as.matrix(y)
```


## Logistic Regression using ttest filtered features
```{r warning=FALSE}
lr.acc.train.t <- c()
lr.acc.val.t <- c()
y.ind = which(names(data.ttest)=='label')
feature.no <- seq(1,20)

for (f in feature.no) {
  lr.acc.train <- c()
  lr.acc.val <- c()
  
  for(i in 1:length(fold)){
    train.df = data.ttest[!cv.377==i, c(seq(1,f), 16836)]
    val.df = data.ttest[cv.377==i, c(seq(1,f), 16836)]
    y.ind = which(names(train.df)=='label')
    
    logistic.model <- glm(label ~ .  , data = train.df, family = binomial(link = 'logit'))
    lr.pred.train <- ifelse(predict(logistic.model, train.df) > 0.5, 1, 0)
    lr.pred.val <- ifelse(predict(logistic.model, val.df) > 0.5, 1, 0)
    lr.acc.train <- c(lr.acc.train, acc(lr.pred.train, train.df[,y.ind]))
    lr.acc.val <- c(lr.acc.val, acc(lr.pred.val, val.df[,y.ind]))
  }
  lr.acc.train.t <- c(lr.acc.train.t, mean(lr.acc.train))
  lr.acc.val.t <- c(lr.acc.val.t, mean(lr.acc.val))
}
```




## SVM linear kernel
```{r}
feature.no <- seq(90, 150, length = 10)
svm.cost <- c(0.001, 0.01, 0.1, 0.3)
svm.acc.n <- length(svm.cost)
svm.acc.m <- length(feature.no)
svm.acc.train.t <- matrix(rep(0, svm.acc.m*svm.acc.n), svm.acc.n, svm.acc.m)
svm.acc.val.t <- matrix(rep(0, svm.acc.m*svm.acc.n), svm.acc.n, svm.acc.m)

for (f in 1:length(feature.no)) {
  lr.acc.train <- c()
  lr.acc.val <- c()
  svm.acc.train <- c()
  svm.acc.val <- c()
  rf.acc.train <- c()
  rf.acc.val <- c()
  for (c in 1:length(svm.cost)) {
    for(i in 1:length(fold)){
      train.df = data.ttest[!cv.377==i, c(seq(1,feature.no[f]),16836)]
      val.df = data.ttest[cv.377==i, c(seq(1,feature.no[f]),16836)]
      y.ind = which(names(train.df)=='label')
      
      svm.model <- svm(x=train.df[,-y.ind], y=train.df[,y.ind],  kernel="linear", type="C-classification", cost = svm.cost[c])
      svm.pred.train <- predict(svm.model, train.df[,-y.ind])
      svm.pred.val <- predict(svm.model, val.df[,-y.ind])
      svm.acc.train <- c(svm.acc.train, acc(svm.pred.train, train.df[,y.ind]))
      svm.acc.val <- c(svm.acc.val, acc(svm.pred.val, val.df[,y.ind]))
    }
    svm.acc.train.t[c,f] <- mean(svm.acc.train)
    svm.acc.val.t[c,f] <- mean(svm.acc.val)
  }
}
```

## SVM radial kernel
```{r}
svm.cost <- c(0.01, 0.1, 0.3, 0.7, 1)
svm.acc.n <- length(svm.cost)
svm.acc.m <- length(feature.no)
svm.acc.train.t.radial <- matrix(rep(0, svm.acc.m*svm.acc.n), svm.acc.n, svm.acc.m)
svm.acc.val.t.radial <- matrix(rep(0, svm.acc.m*svm.acc.n), svm.acc.n, svm.acc.m)

for (f in 1:length(feature.no)) {
  lr.acc.train <- c()
  lr.acc.val <- c()
  svm.acc.train <- c()
  svm.acc.val <- c()
  rf.acc.train <- c()
  rf.acc.val <- c()
  for (c in 1:length(svm.cost)) {
    for(i in 1:length(fold)){
      train.df = data.ttest[!cv.377==i, c(seq(1,feature.no[f]),16836)]
      val.df = data.ttest[cv.377==i, c(seq(1,feature.no[f]),16836)]
      y.ind = which(names(train.df)=='label')
      
      svm.model <- svm(x=train.df[,-y.ind], y=train.df[,y.ind],  kernel="radial", type="C-classification", cost = svm.cost[c])
      svm.pred.train <- predict(svm.model, train.df[,-y.ind])
      svm.pred.val <- predict(svm.model, val.df[,-y.ind])
      svm.acc.train <- c(svm.acc.train, acc(svm.pred.train, train.df[,y.ind]))
      svm.acc.val <- c(svm.acc.val, acc(svm.pred.val, val.df[,y.ind]))
    }
    svm.acc.train.t.radial[c,f] <- mean(svm.acc.train)
    svm.acc.val.t.radial[c,f] <- mean(svm.acc.val)
  }
}
```


```{r warning=FALSE}
rf.acc.train.t <- c()
rf.acc.val.t <- c()

for (f in feature.no) {
  rf.acc.train <- c()
  rf.acc.val <- c()
  
  for(i in 1:length(fold)){
    train.df = data.ttest[!cv.377==i, c(seq(1,f),16836)]
    val.df = data.ttest[cv.377==i, c(seq(1,f),16836)]
    y.ind = which(names(train.df)=='label')
    
    rf.model <- randomForest(label ~ ., data=train.df, importance=TRUE, mtry=round(f^0.5))
    rf.pred.train <- ifelse(predict(rf.model, train.df)>0.5, 1, 0)
    rf.pred.val <- ifelse(predict(rf.model, val.df)>0.5, 1, 0)
    rf.acc.train <- c(rf.acc.train, acc(rf.pred.train, train.df[,y.ind]))
    rf.acc.val <- c(rf.acc.val, acc(rf.pred.val, val.df[,y.ind]))
  }
  
  rf.acc.train.t <- c(rf.acc.train.t, mean(rf.acc.train))
  rf.acc.val.t <- c(rf.acc.val.t, mean(rf.acc.val))
}
```


```{r}
# plot(feature.no, lr.acc.val.t)
# cat('feature no for logistic regression')
# feature.no[which.max(lr.acc.val.t)]
# max(lr.acc.val.t)

plot(feature.no, svm.acc.val.t[1,], type = 'l')
lines(feature.no, svm.acc.val.t[2,], col = 'red')
lines(feature.no, svm.acc.val.t[3,], col = 'blue')
cat('feature no for linear kernel')
feature.no[which.max(svm.acc.val.t)/length(svm.cost)+1]
cat('cost for linear kernal')
svm.cost[which.max(svm.acc.val.t)%%length(svm.cost)+1]
max(svm.acc.val.t)

# plot(feature.no, svm.acc.val.t.radial[1,], type = 'l')
# lines(feature.no, svm.acc.val.t.radial[2,], col = 'red')
# lines(feature.no, svm.acc.val.t.radial[3,], col = 'blue')
# cat('feature no for radial kernel')
# feature.no[which.max(svm.acc.val.t.radial)/length(svm.cost)+1]
# cat('cost for radial kernal')
# svm.cost[which.max(svm.acc.val.t.radial)%%length(svm.cost)+1]
# max(svm.acc.val.t.radial)
# 
# 
# plot(feature.no, rf.acc.val.t)
# cat('feature no for random forest')
# feature.no[which.max(rf.acc.val.t)]
# max(rf.acc.val.t)

```





SEX
