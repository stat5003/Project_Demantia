---
title: "STAT5003Assignment1"
author: "Zezheng Zhang"
date: "26/04/2019"
output: html_document
---

---
title: "STAT5003 Assignment1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Initialize
```{r}
library(ggplot2)
theme_update(plot.title = element_text(hjust = 0.5))

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Exploratory data analysis
### Clean Data
#### DonorInfo
1. Load DonorInfo and drop columns

```{r}
DonorInfo <- read.csv("DonorInformation.csv", header = TRUE, sep = ",", encoding = "UTF-8")
DonorInfo.clean <- DonorInfo[c('donor_id', 'age', 'sex', 'apo_e4_allele', 'education_years', 'age_at_first_tbi', 'num_tbi_w_loc', 'act_demented')]

dim(DonorInfo.clean)
summary(DonorInfo.clean)

ggplot()+geom_histogram(data=data.frame(x=DonorInfo$age_at_first_tbi), bins = 20, aes(x)) + labs(title="Histogram of Age at first TBI", x ="Age at first TBI", y = "Population")
```

2. Deal with N/A and categorical features
age, sex, apo_e4_allele, education_years, age_at_first_tbi, longest_loc_duration, num_tbi_w_loc

Categorical features:
Age: Linear from ages 72-89; binned at 90-94, 95-99, and 100+.
Sex: Gender
apo_e4_allele: Yes - at least one ApoE4 allele; No - no ApoE4 allele present
longest_loc_duration: Ordinal score of length of loss of consciousness: 0 (none or unknown), 1 (a few sec or <), 2 (min or <), 3 (1-2 min), 4 (3-5 min), 5 (6-9 min), 6 (10 min-1 hr), and 7 (> 1 hr).

Numerical features:
education_years: Number of years of education completed
age_at_first_tbi: Age at which first TBI with loss of consciousness was reported.
num_tbi_w_loc: Number of recorded TBI, ranging from 0 (control) to 3.

```{r}
#clean age
summary(DonorInfo$age)
age <- as.character(DonorInfo$age) 
age[DonorInfo$age=='90-94'] <- 92
age[DonorInfo$age=='95-99'] <- 97
age[DonorInfo$age=='100+'] <- 100
age <- as.numeric(age)

ggplot()+geom_histogram(data=data.frame(x=age), bins = 20, aes(x)) + labs(title="Histogram of age", x ="Age", y = "Population")
summary(age)
DonorInfo.clean$age <- age

#clean apo_e4_allele
summary(DonorInfo$apo_e4_allele)
apo_e4_allele <- as.numeric(DonorInfo$apo_e4_allele)-1
#0->N 1->N/A 2->Y
#all N/A to N
apo_e4_allele[apo_e4_allele==1] <- 0
apo_e4_allele[apo_e4_allele==2] <- 1
DonorInfo.clean$apo_e4_allele <- apo_e4_allele

DonorInfo.clean$sex <- as.numeric(DonorInfo$sex)-1
DonorInfo.clean$act_demented <- as.numeric(DonorInfo$act_demented)-1

```

3. Check cleaned data

```{r}
summary(DonorInfo.clean)
```

# create 10 fold cross validation based on patients
```{r}
library(caret)
fold <- createFolds(DonorInfo.clean$act_demented, k=10)
cv = rep(0,length(DonorInfo.clean$act_demented))
for (i in 1:10) {
  cv[fold[[i]]] = i
}
DonorInfo.clean['cv'] = cv
table(DonorInfo.clean$cv)
```


#### ProteinInfo
1. Load ProteinInfo and remove redundant columns

```{r}
ProteinInfo.raw <- read.csv("ProteinAndPathologyQuantifications.csv", header = TRUE, sep = ",", encoding = "UTF-8")
head(ProteinInfo.raw)
ProteinInfo <- ProteinInfo.raw[,c(-2,-4)]
head(ProteinInfo)
```

```{r}
colSums(is.na(ProteinInfo))

p1 <-ggplot()+geom_density(data=data.frame(x=ProteinInfo$isoprostane_pg_per_mg), na.rm = T, aes(x)) + labs(title="Density of isoprostane_pg_per_mg", x ="isoprostane_pg_per_mg", y = "Density")
p2 <-ggplot()+geom_density(data=data.frame(x=ProteinInfo$ihc_tau2_ffpe), na.rm = T, aes(x)) + labs(title="Density of ihc_tau2_ffpe", x ="ihc_tau2_ffpe", y = "Density")
p3 <-ggplot()+geom_density(data=data.frame(x=ProteinInfo$ihc_at8_ffpe), na.rm = T, aes(x)) + labs(title="Density of ihc_at8_ffpe", x ="ihc_at8_ffpe", y = "Density")

multiplot(p1, p2, p3, cols=1)

#replace na with column mean
for (i in seq(ncol(ProteinInfo))) {
  ind <- is.na(ProteinInfo[,i])
  ProteinInfo[ind, i] <- median(ProteinInfo[,i], na.rm = T)
}

#remove the column with 239 N/A
ProteinInfo1 <- ProteinInfo[,-which(colnames(ProteinInfo)=="isoprostane_pg_per_mg")]

colSums(is.na(ProteinInfo1))
```

#merge ProteinInfo with DonorInfo

```{r}
data1 <- merge(x = DonorInfo.clean, y = ProteinInfo1, by = "donor_id", all = TRUE)
head(data1)
```

#### FPKM with column sample information
1. read csv

```{r}
colsamp <- read.csv("columns-samples.csv", header = TRUE, sep = ",", encoding = "UTF-8")
dim(colsamp)
head(colsamp)
fpkm <- read.csv("fpkm_table_unnormalized.csv", header = TRUE, sep = ",", encoding = "UTF-8")
dim(fpkm)
fpkm[1:5,1:10]

#summary(colsamp[colsamp$structure_name=='temporal neocortex',]$structure_color)
```

2. remove redundant ids in colsamp

```{r}
colsamp <- colsamp[,c(1,2,8)]
head(colsamp)
```

3. merge colsamp with data1

```{r}
data2 <- merge(x = colsamp, y = data1, by = c("donor_id", "structure_id"))
dim(data2)
head(data2)
colnames(data2)

# reorder the dataframe based on rnaseq profile id
data2.new <- data2[order(data2[,'rnaseq_profile_id']),]
head(data2.new)

```

4. transpose fpkm

```{r}
fpkm1 <- as.data.frame(t(fpkm))[-1,]
colnames(fpkm1)<-fpkm[,1]
fpkm1[1:5, 1:10]
```

5. combine fpkm1 with data2.new to produce data.final

```{r}
data3 <- cbind(data2.new, fpkm1)
rownames(data3) <- rownames(fpkm1)

#remove ids
data.final <- data3[,-(1:3)]

data.final[1:5, 1:10]

```

```{r}
#plot(density(rowSums(fpkm1==0)))
#plot(density(colSums(fpkm1==0)))
#plot((rowSums(fpkm1==0)))
#plot(sort(colSums(fpkm1==0)))
#((rowSums(fpkm1==0)))
#((colSums(fpkm1==0)))
```

```{r}
# save cv info
cv.377 <- data.filter1['cv']

# save y values
y <- as.matrix(data.filter1['act_demented'])
```

## feature filtering

1. remove features with more than 250 0s. 
```{r}
# Check if there is any 0 columns
ind.zero <- which(colSums(dataF==0)>250)
data.filter1 <- data.final[, -ind.zero]
sum(colSums(dataFF)==0)
dim(data.filter1)
```

2. remove features with less than 0.001 correaltion with y 
```{r}
# correlation with y
feature.cor <- cor(x, y)
lowcorind <- which(abs(feature.cor)<0.01)
data.filter2 <- data.filter1[,-lowcorind]

# dimension of the filtered data
dim(data.filter2)

# sample of filtered feature names
colnames(data.filter1)[lowcorind[1:20]]
```

3. remove features with t test
```{r}
# perform a t-test
feature.pvalues <- c()
for(i in 1:(ncol(data.filter2))) {
  feature.pvalues <- c(feature.pvalues, t.test(data.filter2[,i], y)$p.value)
}
names(feature.pvalues) <- colnames(data.filter2)

ttest.filter.ind <- which(feature.pvalues<0.05)

data.filter3 <- data.filter2[, -ttest.filter.ind]
dim(data.filter3)
```

```{r}
which(colnames(data.filter3)=="act_demented")
which(colnames(data.filter3)=="cv")
```


```{r warning = FALSE}
data.numeric <- data.filter3
data.pca.scale <- prcomp(data.numeric, scale = T)
# biplot(data.pca.scale, cex = 0.5)
```
```{r}
pca.df <- as.data.frame(data.pca.scale$x)
pca.df['label']=as.factor(y)
ggplot(pca.df, aes(PC1, PC2, colour = label)) + geom_point() 
```


```{r}
library(ggplot2)
tot.var <- sum(data.pca.scale$sdev^2)
var.explained <- data.frame(pc = seq(1:length(data.pca.scale$sdev)), var.explained  = data.pca.scale$sdev^2/tot.var ) 
ggplot(var.explained, aes(pc, var.explained)) + geom_point()+ ggtitle("variance explained vs pc")
ggplot(var.explained, aes(pc, cumsum(var.explained))) + geom_point()+ ggtitle("cumsum var explained vs pc")
```

# logistic regression on PCA
```{r warning=FALSE}
lr.acc.train.pc <- c()
lr.acc.test.pc <- c()

for (pc in seq(30,300, length=100)) {
  lr.acc.train <- c()
  lr.acc.test <- c()
  for(i in 1:length(fold)){
    train.df = pca.df[!cv==i, c(seq(1,pc),378)]
    val.df = pca.df[cv==i, c(seq(1,pc),378)]
    y.ind = which(names(train.df)=='label')
    
    logistic.model <- glm(label ~ .  , data = train.df, family = binomial(link = 'logit'))
    
    # use fitted value to classify samples
    lr.pred.train <- ifelse(predict(logistic.model, train.df) > 0.5, 1, 0)
    lr.acc.train <- c(lr.acc.train, sum(lr.pred.train == train.df[,y.ind]) / nrow(train.df))
    
    lr.pred.test <- ifelse(predict(logistic.model, val.df) > 0.5, 1, 0)
    lr.acc.test <- c(lr.acc.test, sum(lr.pred.test == val.df[,y.ind]) / nrow(val.df))
  }
  lr.acc.train.pc <- c(lr.acc.train.pc, mean(lr.acc.train))
  lr.acc.test.pc <- c(lr.acc.test.pc, mean(lr.acc.test))
}

```

```{r}
plot(lr.acc.test.pc)
which.max(lr.acc.test.pc)
max(lr.acc.test.pc)
```




```{r}
library(Rtsne)
data.Rtsne <- Rtsne(data.numeric)
data.Rtsne.df <- data.frame(dim1 = data.Rtsne$Y[,1], dim2 = data.Rtsne$Y[,2], labels = as.factor(y))
p1 <- ggplot(data.Rtsne.df, aes(dim1, dim2, col = labels)) + geom_point() + ggtitle("Perplexity = 30")
p1
```


```{r}
x.pca <- data.pca.scale$x
# y <- y
```


### Ridge regression
```{r}
library(glmnet)
# set the range of lambda values to be tested.
grid <- 10^seq(8,-2, length=100)

ridge.mod <- glmnet(x.pca, as.matrix(y), alpha=0, lambda=grid, standardize = TRUE)
dim(coef(ridge.mod))
plot(ridge.mod, xvar="lambda", label=TRUE)
set.seed(1)
cv.out <- cv.glmnet(x.pca, as.matrix(y), alpha=0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min 
bestlam.ridge
```

### Lasso regression
```{r}
## Lasso model 
lasso.mod <- glmnet(x, y, alpha=1, lambda=grid)
dim(coef(lasso.mod))
plot(lasso.mod, "lambda", label=TRUE)

set.seed (1)
# Using cross-validation for Lasso to find the best lambda (based on cvm "mean cross-validated error")
cv.lasso <- cv.glmnet (x, y, alpha=1)
plot(cv.lasso)
bestlam.lasso <- cv.lasso$lambda.min

```

```{r}
# Ridge for feature selection?
ridge.coef <- predict(ridge.mod, type="coefficients", s=bestlam.ridge)[1:50,]
#sort(abs(ridge.coef), decreasing = T)[1:10]
ridge.coef

# Lasso for feature selection
lasso.coef=predict(lasso.mod, type="coefficients", s=bestlam.lasso)
#sort(abs(lasso.coef), decreasing = T)[1:10]
colnames(x)[which(lasso.coef!=0)]

```


## logistic regression
```{r warning=FALSE}
lr.acc.train <- vector()
lr.acc.test <- vector()
for(i in 1:length(fold)){
  logistic.model <- glm(y ~ .  , data = train.df[fold[[i]],], family = binomial(link = 'logit'))
  # use fitted value to classify samples
  lr.pred.train <- ifelse(predict(logistic.model, train.df[,-1]) > 0.5, 1, 0)
  # calculate classification accuracy (in percentage %)
  lr.acc.train[i] <- sum(logit.decision == y.train) / nrow(train.df) * 100
  
  lr.pred.test <- ifelse(predict(logistic.model, test.df[,-1]) > 0.5, 1, 0)
  lr.acc.test[i] <- sum(lr.pred.test == y.test) / length(y.test) * 100
}

mean(lr.acc.train)
mean(lr.acc.test)
```


## linear model 
not appropriate for classification
```{r warning=FALSE}

linear.acc.train <- vector()
linear.acc.test <- vector()
for(i in 1:length(fold)){
  linear.model <- lm(y ~ .  , data = train.df[fold[[i]],])
  # summary(linear.model)
  linear.pred.train <- ifelse(predict(linear.model, train.df[,-1]) > 0.5, 1, 0)
  linear.acc.train[i] <- sum(logit.decision == y.train) / nrow(train.df) * 100
  
  linear.pred.test <- ifelse(predict(linear.model, test.df[,-1]) > 0.5, 1, 0)
  linear.acc.test[i] <- sum(linear.pred.test == y.test) / length(y.test) * 100
}

mean(linear.acc.train)
mean(linear.acc.test)
```

## knn
```{r}
library(class)

knn.acc.train <- vector()
knn.acc.test <- vector()

for(i in 1:length(fold)){
  knn.pred.train <- knn(train.df[fold[[i]],-1], train.df[fold[[i]],-1], train.df[fold[[i]],]$y, k=5)
  knn.pred.test <- knn(train.df[fold[[i]],-1], test.df[,-1], train.df$y[fold[[i]]], k=5)
  knn.acc.train[i] <- sum(knn.pred.train == y.train) / nrow(train.df) * 100
  knn.acc.test[i] <- sum(knn.pred.test == y.test) / nrow(test.df) * 100
}

mean(knn.acc.train)
mean(knn.acc.test)
```

## lda
Problem with lda is that it does not take collinear inputs, where in our data it is highly probable in multiple features
```{r}
lda.acc.train <- vector()
lda.acc.test <- vector()

for(i in 1:length(fold)){
  # Train the lda model 
  lda.model <- MASS::lda(y ~ .  , data = train.df[fold[[i]],1:30])
  lda.fitted <- predict(lda.model, train.df[,1:30])$posterior[,2]
  # use fitted value to classify samples
  lda.pred.train <- ifelse(lda.fitted > 0.5, 1, 0)
  # calculate classification accuracy (in percentage %)
  lda.acc.train[i] <- sum(lda.pred.train == y.train) / length(y.train) * 100
  lda.pred.test <- ifelse(predict(lda.model, test.df[,1:30])$posterior[,2] > 0.5, 1, 0)
  lda.acc.test[i] <- sum(lda.pred.test == y.test)/length(y.test) * 100
}

mean(lda.acc.train)
mean(lda.acc.test)
```

## random forest
```{r warning=FALSE}
library(randomForest)

set.seed(1)

rf.acc.train <- vector()
rf.acc.test <- vector()

for(i in 1:length(fold)){
  # Random forest for classification
  rf.model <- randomForest(y ~ ., data=train.df[,1:30], importance=TRUE, mtry=1)
  print(rf.model)
  rf.model$importance
  rf.pred.train <- ifelse(rf.model$predicted>0.5, 1, 0)
  rf.pred.test <- ifelse(predict(rf.model, test.df[,1:30])>0.5, 1, 0) 
  
  rf.acc.train[i] <- sum(rf.pred.train == y.train) / length(y.train) * 100
  rf.acc.test[i] <- sum(rf.pred.test == y.test)/length(y.test) * 100
}

mean(rf.acc.train)
mean(rf.acc.test)
```





